# -*- coding: utf-8 -*-
"""修正版.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11nYClvIF1fTvjAidyWDOmyPYtkrencap
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import numpy as np
from sklearn.naive_bayes import BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
import sklearn.tree as tree
from sklearn import svm
df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/卒論/heart_failure_clinical_records_dataset.csv')

df['FU']=0

for i in range(len(df)):
  df.at[i,'FU']=int(df['time'][i]/30)

df=df.drop('time',axis=1)

#欠損値をknnで補完
from sklearn.impute import KNNImputer
df_imputed = pd.DataFrame(KNNImputer(n_neighbors=2).fit_transform(df))
df_imputed.columns = df.columns
df=df_imputed

h_list=['creatinine_phosphokinase', 'ejection_fraction','platelets', 'serum_creatinine', 'serum_sodium',]

#標準化
from sklearn.preprocessing import StandardScaler
sc = StandardScaler().fit(df[h_list])
scaled_df = pd.DataFrame(sc.transform(df[h_list]), columns=h_list, index=df.index)
df.update(scaled_df)

#csv_writer = df.to_csv('/content/drive/MyDrive/Colab Notebooks/卒論/pre-data.csv')

y=df['DEATH_EVENT']
df2=df.drop('DEATH_EVENT',axis=1)

import itertools
pair_list=[]

for pair in itertools.combinations(list(df2),10):
  pair_list.append(pair)

import itertools
pair_list=[]

for i in range(1,13):
  for pair in itertools.combinations(list(df2),i):
    pair_list.append(pair)

print(len(pair_list))

acc_score_list=[] #accuracy
tp_score_list=[]  #true positive rate
f1_score_list=[] #f1_score
tn_score_list=[] #true negative rate

for pair in pair_list:
  l=list(pair)
  x=df.loc[:,l]
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)
  #test_len=len(y_test)

  #Random Forest
  rf = RandomForestClassifier(n_estimators=20,max_depth=10) 
  rf.fit(x_train, y_train)
  y_pred = rf.predict(x_test)
  acc = accuracy_score(y_test, y_pred)
  f1=f1_score(y_test, y_pred) 
  tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
  f1_score_list.append(f1)
  acc_score_list.append(acc)
  tp_score_list.append(tp/(tp+fp))
  tn_score_list.append(tn/(fn+tn))
  
  #svm
  clf = svm.SVC(gamma=0.01, C=100.)
  clf.fit(x_train, y_train)
  y_pred = clf.predict(x_test)
  acc = accuracy_score(y_test, y_pred)
  f1=f1_score(y_test, y_pred) 
  tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
  f1_score_list.append(f1)
  acc_score_list.append(acc)
  tp_score_list.append(tp/(tp+fp))
  tn_score_list.append(tn/(fn+tn))

  #ロジスティック回帰
  lr = LogisticRegression() 
  lr.fit(x_train, y_train) 
  y_pred = lr.predict(x_test)
  acc = accuracy_score(y_test, y_pred)
  f1=f1_score(y_test, y_pred) 
  tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
  f1_score_list.append(f1)
  acc_score_list.append(acc)
  tp_score_list.append(tp/(tp+fp))
  tn_score_list.append(tn/(fn+tn))

  #xgboost
  model = XGBClassifier(max_depth=10, eta=0.01)
  eval_set = [(x_test, y_test)]
  model.fit(x_train, y_train, eval_set=eval_set, verbose=True)
  y_pred1 = model.predict_proba(x_test)

  y_pred2=[]
  th=0.5

  for i in range(len(y_pred1)):
    if y_pred1[i][0]>th:
      y_pred2.append(0)
    else:
      y_pred2.append(1)

  acc = accuracy_score(y_test, y_pred2)
  f1=f1_score(y_test, y_pred2) 
  tn, fp, fn, tp = confusion_matrix(y_test, y_pred2, labels=[0, 1]).ravel()
  f1_score_list.append(f1)
  acc_score_list.append(acc)
  tp_score_list.append(tp/(tp+fp))
  tn_score_list.append(tn/(fn+tn))

  # Bernoulli naive Bayes
  BNB_model = BernoulliNB(alpha=1.0,          # ラプラススムージングのパラメーター（ゼロ頻度問題対策）
                        binarize=0.0,       # 2値化の閾値（閾値値を超える値を1、それ以外を0に分類）
                        fit_prior=True,     # Trueの場合はクラスの事前確率を計算
                        class_prior=None)   # 上記Trueの場合、各クラスの事前確率をタプル形式で指定   

  BNB_model.fit(x_train,y_train)
  BNB_model.get_params(deep=True)
  Y_pred_BNB = BNB_model.predict(x_test)
  acc = accuracy_score(y_test, y_pred)
  f1=f1_score(y_test, y_pred) 
  tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
  f1_score_list.append(f1)
  acc_score_list.append(acc)
  tp_score_list.append(tp/(tp+fp))
  tn_score_list.append(tn/(fn+tn))

  #KNN
  knn = KNeighborsClassifier(n_neighbors=2)
  knn.fit(x_train, y_train)
  y_pred = knn.predict(x_test)
  acc = accuracy_score(y_test, y_pred)
  f1=f1_score(y_test, y_pred)
  tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
  f1_score_list.append(f1)
  acc_score_list.append(acc)
  tp_score_list.append(tp/(tp+fp))
  tn_score_list.append(tn/(fn+tn))

  #決定木
  model = tree.DecisionTreeClassifier(max_depth=2, random_state=1)
  model.fit(x_train, y_train)
  y_pred = model.predict(x_test)
  acc = accuracy_score(y_test, y_pred)
  f1=f1_score(y_test, y_pred)
  tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
  f1_score_list.append(f1)
  acc_score_list.append(acc)
  tp_score_list.append(tp/(tp+fp))
  tn_score_list.append(tn/(fn+tn))